{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RAG\n",
    "An LLM app using LangChain & Chroma. Built from this [LangChain RAG tutorial](https://python.langchain.com/v0.2/docs/tutorials/rag/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain_community langchain_chroma\n",
    "%pip install -qU langchain-openai\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "The indexing phase of creating a RAG application includes loading, chunking, and indexing the contents of the source material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# 1. Load\n",
    "# WebBaseLoader is a Document Loader which uses urllib to load HTML from web URLs and \n",
    "# BeautifulSoup to parse it to text. BS keyword arguments are passed in to limit the\n",
    "# classes loaded: Only keep post title, headers, and content from the full HTML.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. Split\n",
    "# Split the document into chunks of 1000 characters with 200 characters of overlap between chunks.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3. Store\n",
    "# Store splits and their embeddings in a vector store. \n",
    "vectorstore = Chroma.from_documents(splits, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval & Generation\n",
    "For this exercise, create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Retrieve\n",
    "# A \"retriever\" is an object that returns Documents given a text query\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "# 5. Generate\n",
    "# Generate a response to the question using the retrieved documents.\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create_stuff_documents_chain: \n",
    "# specifies how retrieved context is fed into a prompt and LLM. \n",
    "# In this case, we will \"stuff\" the contents into the prompt -- i.e., we will include all \n",
    "# retrieved context without any summarization or other processing. It largely implements our \n",
    "# above rag_chain, with input keys context and input-- it generates an answer using retrieved \n",
    "# context and query.\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# create_retrieval_chain:\n",
    "#  adds the retrieval step and propagates the retrieved context through \n",
    "# the chain, providing it alongside the final answer. It has input key input, and includes input, \n",
    "# context, and answer in its output.\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "query = input(\"> \")\n",
    "\n",
    "response = rag_chain.invoke({\"input\": query})\n",
    "# e.g. \"What is Task Decomposition?\"\n",
    "# e.g. \"How are tools used in LLMs?\"\n",
    "\n",
    "print(response[\"answer\"])\n",
    "print()\n",
    "\n",
    "for counter, document in enumerate(response[\"context\"], start=1):\n",
    "    print(f\"Document {counter}:\")\n",
    "    print(document)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
